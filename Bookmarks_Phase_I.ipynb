{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1:\n",
    "## Process and examine google bookmarks json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Create a copy of google chrome bookmark file into working folder data subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\data\\\\Bookmarks.json'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the google chrome bookmarks file into the current folder data subfolder\n",
    "\n",
    "from platform import system\n",
    "from os import environ, path, curdir\n",
    "from shutil import copyfile\n",
    "import datetime\n",
    "        \n",
    "def convertChrometime(dtms):\n",
    "    seconds, micros = divmod(dtms, 1000000)\n",
    "    days, seconds = divmod(seconds, 86400)\n",
    "    return datetime.datetime(1601, 1, 1) + datetime.timedelta(days, seconds, micros)\n",
    "#print( convertChrometime(13024882639633631).strftime( '%a, %d %B %Y %H:%M:%S %Z' ) )\n",
    "\n",
    "def get_chrome_bookmarks_path(syst):\n",
    "    chrome_bookmarks = ''\n",
    "    if syst == \"Darwin\":\n",
    "        chrome_bookmarks = path.expanduser(\"~/Library/Application Support/Google/Chrome/Default/Bookmarks\")\n",
    "    elif syst == \"Linux\":\n",
    "        chrome_bookmarks = path.expanduser(\"~/.config/google-chrome/Default/Bookmarks\")\n",
    "    elif syst == \"Windows\":\n",
    "        chrome_bookmarks = environ[\"LOCALAPPDATA\"] + r\"\\Google\\Chrome\\User Data\\Default\\Bookmarks\"\n",
    "    else:\n",
    "        print('Your system (\"{}\") is not handled. File path requested.'.format(syst))\n",
    "        chrome_bookmarks = input(\"Please provide the full path to Chrome Bookmarks file\")\n",
    "\n",
    "    if len(chrome_bookmarks)>0:\n",
    "        return chrome_bookmarks\n",
    "    else:\n",
    "        exit(1)\n",
    "\n",
    "BKM_master = get_chrome_bookmarks_path(system())\n",
    "\n",
    "BKM_file = \"Bookmarks.json\"\n",
    "BKM_copy = path.join(curdir, \"data\", BKM_file)\n",
    "\n",
    "copyfile(BKM_master, BKM_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Process the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The bookmarks file has a deeply nested structure even before addition of subfolders:\n",
    "This turns out to be a problem on its own as there are to my knowledge no straightforward way /\n",
    "to output a json file into a pandas frame in a generic way: one has to know the structure and the key labels. /\n",
    "    \n",
    "The following command lines use the line indentation (3 char) to get some sense of the structure: /\n",
    "\n",
    "Level 1:\n",
    "```\n",
    "> grep -E '^ {3}\"' Bookmarks.json\n",
    "   \"checksum\": \"fbefde58fad3cc708c0beffbbfdfb0a7\",\n",
    "   \"roots\": {\n",
    "   \"version\": 1\n",
    "```\n",
    "\n",
    "Level 2:\n",
    "```\n",
    "> grep -E '^ {6}\"' Bookmarks.json\n",
    "      \"bookmark_bar\": {\n",
    "      \"other\": {                                     **# \"Other bookmarks\" is in there**\n",
    "      \"sync_transaction_version\": \"1320\",\n",
    "      \"synced\": {\n",
    "```\n",
    "\n",
    "Level 3:\n",
    "```      \n",
    "> grep -E '^ {9}\"' Bookmarks.json\n",
    "         \"children\": [ {\n",
    "         \"date_added\": \"13109116088391906\",\n",
    "         \"date_modified\": \"13169923254937857\",\n",
    "         \"id\": \"1\",\n",
    "         \"name\": \"Bookmarks bar\",\n",
    "         \"type\": \"folder\"\n",
    "         \"children\": [ {\n",
    "         \"date_added\": \"13109116088391910\",\n",
    "         \"date_modified\": \"13163607256316120\",\n",
    "         \"id\": \"2\",\n",
    "         \"name\": \"Other bookmarks\",                  **# Found \"Other bookmarks\": children needed**\n",
    "         \"type\": \"folder\"\n",
    "         \"children\": [ {\n",
    "         \"date_added\": \"13109116088391912\",\n",
    "         \"date_modified\": \"13145377539948572\",\n",
    "         \"id\": \"3\",\n",
    "         \"name\": \"Mobile bookmarks\",\n",
    "         \"type\": \"folder\"\n",
    "```\n",
    "\n",
    "What are the values under 'Type'?\n",
    "```             \n",
    "> grep -e 'type\\\"' Bookmarks.json| tr -s \\ | sort | uniq\n",
    " \"type\": \"folder\"\n",
    " \"type\": \"url\",\n",
    "```\n",
    "Nice to know!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLD Perso 13109174777723357\n",
      "dict_keys(['sync_transaction_version', 'children', 'id', 'date_added', 'date_modified', 'name', 'type'])\n",
      "FLD News 13109174777705078\n",
      "dict_keys(['sync_transaction_version', 'children', 'id', 'date_added', 'date_modified', 'name', 'type'])\n",
      "FLD Accounts 13151775038046138\n",
      "dict_keys(['sync_transaction_version', 'children', 'id', 'date_added', 'date_modified', 'name', 'type'])\n",
      "FLD CUNY 13109116258767161\n",
      "dict_keys(['sync_transaction_version', 'children', 'id', 'date_added', 'date_modified', 'name', 'type'])\n",
      "FLD Resources & Tools 13109116258767360\n",
      "dict_keys(['sync_transaction_version', 'children', 'id', 'date_added', 'date_modified', 'name', 'type'])\n",
      "FLD Courses & Textbooks 13109116258767557\n",
      "dict_keys(['sync_transaction_version', 'children', 'id', 'date_added', 'date_modified', 'name', 'type'])\n",
      "FLD Science sites & tools 13109116258768141\n",
      "dict_keys(['sync_transaction_version', 'children', 'id', 'date_added', 'date_modified', 'name', 'type'])\n",
      "FLD Misc 13109116258768527\n",
      "dict_keys(['sync_transaction_version', 'children', 'id', 'date_added', 'date_modified', 'name', 'type'])\n",
      "FLD Progg 13109174778060091\n",
      "dict_keys(['sync_transaction_version', 'children', 'id', 'date_added', 'date_modified', 'name', 'type'])\n",
      "FLD Jobs 13109174777774261\n",
      "dict_keys(['sync_transaction_version', 'children', 'id', 'date_added', 'date_modified', 'name', 'type'])\n",
      "FLD People 13109174777787223\n",
      "dict_keys(['sync_transaction_version', 'children', 'id', 'date_added', 'date_modified', 'name', 'type'])\n",
      "BKM Amazon Student 13052003813434881\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'meta_info', 'date_added', 'type'])\n",
      "BKM NYT Chronicle 13063834756373080\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'date_added', 'type'])\n",
      "BKM BetterWorldBooks.com 12922401002026674\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'meta_info', 'date_added', 'type'])\n",
      "BKM Weather Forecast New York City - Foreca.com 13040939957191370\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'meta_info', 'date_added', 'type'])\n",
      "BKM Local time in France – Paris 13015966990995159\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'date_added', 'type'])\n",
      "BKM National Hurricane Center 13028410021875400\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'meta_info', 'date_added', 'type'])\n",
      "BKM weather.gov 13129671690278805\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'meta_info', 'date_added', 'type'])\n",
      "BKM USGS - Earthquakes 13013887684506372\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'date_added', 'type'])\n",
      "BKM Météo-France 13109352631895116\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'meta_info', 'date_added', 'type'])\n",
      "BKM Google Search Operators - Google Guide 13052002899273884\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'date_added', 'type'])\n",
      "BKM (3) Introduction to animation  Introduction to animation curves  Khan Academy 13087677930052652\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'meta_info', 'date_added', 'type'])\n",
      "BKM Greek Unicode Entities 13110665916780209\n",
      "dict_keys(['sync_transaction_version', 'name', 'url', 'id', 'date_added', 'type'])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# This is Level 1: dict_keys(['version', 'roots', 'checksum'])\n",
    "data.keys()            \n",
    "\n",
    "# Level 2:  dict_keys(['sync_transaction_version', 'bookmark_bar', 'other', 'synced'])\n",
    "data['roots'].keys()\n",
    "\n",
    "# Level 3:  dict_keys(['date_added', 'date_modified', 'id', 'type', 'children', 'name'])\n",
    "data['roots']['other'].keys()\n",
    "\n",
    "# Level 4:  dict_keys(['date_added', 'date_modified', 'id', 'type', 'children', 'name']):\n",
    "'''\n",
    "\n",
    "subf = len(data['roots']['other']['children'])\n",
    "#print(subf)\n",
    "\n",
    "location = ''.join('Other bookmarks')\n",
    "location = location.join('|')\n",
    "\n",
    "for f in range(subf):\n",
    "    other_level = data['roots']['other']['children'][f]\n",
    "    label = other_level['name']\n",
    "    '''\n",
    "    difference in the keys btw fldr and bmrk:\n",
    "    FLD: dict_keys(['sync_transaction_version', 'date_modified', 'type', 'date_added', 'id', 'name', 'children'])\n",
    "    BKM: dict_keys(['sync_transaction_version', 'meta_info', 'url', 'type', 'date_added', 'id', 'name'])\n",
    "   '''\n",
    "    \n",
    "    try:\n",
    "        has_children = bool(other_level['children'])\n",
    "        print(\"FLD {} {}\\n{}\".format(label, other_level['date_added'], other_level.keys()))  # 'Other bookmarks' subfolder name\n",
    "    except KeyError:\n",
    "        print(\"BKM {} {}\\n{}\".format(label, other_level['date_added'], other_level.keys()))  # 'Other bookmarks' bookmark item name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json 'whole upload ' testing:\n",
    "\n",
    "- Need to keep track of folder/subfolder names as this will be one of the classification features to be tested\n",
    "- Ideally, I want the entire contents of the 'Other bookmarks' folder into a df & only then proceed with the data cleaning and selection.\n",
    "\n",
    "** From my search in varous programming spheres, finding a solution to this particular problem \n",
    "would help a lot of people! **\n",
    "\n",
    "Getting closer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(BKM_copy, encoding='utf-8') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = json_normalize(data['roots']['other']['children'], \n",
    "                    meta=[['children', 'type'], ['name', 'url', 'id', 'meta_info', 'date_added', 'type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>children</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_modified</th>\n",
       "      <th>id</th>\n",
       "      <th>meta_info</th>\n",
       "      <th>name</th>\n",
       "      <th>sync_transaction_version</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'sync_transaction_version': '1', 'children':...</td>\n",
       "      <td>13109174777723357</td>\n",
       "      <td>13117039265083786</td>\n",
       "      <td>1203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Perso</td>\n",
       "      <td>1</td>\n",
       "      <td>folder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'sync_transaction_version': '1', 'name': '10...</td>\n",
       "      <td>13109174777705078</td>\n",
       "      <td>13170097765743670</td>\n",
       "      <td>1137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "      <td>folder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'sync_transaction_version': '1', 'name': 'Vi...</td>\n",
       "      <td>13151775038046138</td>\n",
       "      <td>13163106408774757</td>\n",
       "      <td>3688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Accounts</td>\n",
       "      <td>1</td>\n",
       "      <td>folder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'sync_transaction_version': '1', 'children':...</td>\n",
       "      <td>13109116258767161</td>\n",
       "      <td>13167924695837332</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CUNY</td>\n",
       "      <td>1</td>\n",
       "      <td>folder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'sync_transaction_version': '1', 'name': '10...</td>\n",
       "      <td>13109116258767360</td>\n",
       "      <td>13169571777752619</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Resources &amp; Tools</td>\n",
       "      <td>1</td>\n",
       "      <td>folder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'sync_transaction_version': '1', 'name': 'Bi...</td>\n",
       "      <td>13109116258767557</td>\n",
       "      <td>13164750190698910</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Courses &amp; Textbooks</td>\n",
       "      <td>1</td>\n",
       "      <td>folder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'sync_transaction_version': '1', 'children':...</td>\n",
       "      <td>13109116258768141</td>\n",
       "      <td>13169487040373303</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Science sites &amp; tools</td>\n",
       "      <td>1</td>\n",
       "      <td>folder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[{'sync_transaction_version': '1', 'children':...</td>\n",
       "      <td>13109116258768527</td>\n",
       "      <td>13146764550541136</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Misc</td>\n",
       "      <td>1</td>\n",
       "      <td>folder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[{'sync_transaction_version': '1', 'children':...</td>\n",
       "      <td>13109174778060091</td>\n",
       "      <td>13167438541781746</td>\n",
       "      <td>1617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Progg</td>\n",
       "      <td>1</td>\n",
       "      <td>folder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[{'sync_transaction_version': '1', 'children':...</td>\n",
       "      <td>13109174777774261</td>\n",
       "      <td>13168624772525920</td>\n",
       "      <td>1399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jobs</td>\n",
       "      <td>1</td>\n",
       "      <td>folder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[{'sync_transaction_version': '1', 'children':...</td>\n",
       "      <td>13109174777787223</td>\n",
       "      <td>13169689046946971</td>\n",
       "      <td>1455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>People</td>\n",
       "      <td>1</td>\n",
       "      <td>folder</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13052003813434881</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2481</td>\n",
       "      <td>{'last_visited_desktop': '13134580082653860'}</td>\n",
       "      <td>Amazon Student</td>\n",
       "      <td>1</td>\n",
       "      <td>url</td>\n",
       "      <td>http://www.amazon.com/gp/student/signup/info/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13063834756373080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NYT Chronicle</td>\n",
       "      <td>1</td>\n",
       "      <td>url</td>\n",
       "      <td>http://chronicle.nytlabs.com/?keyword=Bush.Nix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12922401002026674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2483</td>\n",
       "      <td>{'last_visited_desktop': '13170027462729742'}</td>\n",
       "      <td>BetterWorldBooks.com</td>\n",
       "      <td>1203</td>\n",
       "      <td>url</td>\n",
       "      <td>http://www.betterworldbooks.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13040939957191370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2484</td>\n",
       "      <td>{'last_visited_desktop': '13166057563293141'}</td>\n",
       "      <td>Weather Forecast New York City - Foreca.com</td>\n",
       "      <td>1</td>\n",
       "      <td>url</td>\n",
       "      <td>http://www.foreca.com/United_States/New_York/N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13015966990995159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local time in France – Paris</td>\n",
       "      <td>1</td>\n",
       "      <td>url</td>\n",
       "      <td>http://www.timeanddate.com/worldclock/city.htm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13028410021875400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2487</td>\n",
       "      <td>{'last_visited_desktop': '13164602768571434'}</td>\n",
       "      <td>National Hurricane Center</td>\n",
       "      <td>1</td>\n",
       "      <td>url</td>\n",
       "      <td>http://www.nhc.noaa.gov/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13129671690278805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2911</td>\n",
       "      <td>{'last_visited_desktop': '13166057638023431'}</td>\n",
       "      <td>weather.gov</td>\n",
       "      <td>1</td>\n",
       "      <td>url</td>\n",
       "      <td>http://www.weather.gov/phi/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13013887684506372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USGS - Earthquakes</td>\n",
       "      <td>1</td>\n",
       "      <td>url</td>\n",
       "      <td>http://earthquake.usgs.gov/earthquakes/?source...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13109352631895116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2516</td>\n",
       "      <td>{'last_visited_desktop': '13164416977631716'}</td>\n",
       "      <td>Météo-France</td>\n",
       "      <td>1</td>\n",
       "      <td>url</td>\n",
       "      <td>http://www.meteofrance.com/previsions-meteo-fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             children         date_added  \\\n",
       "0   [{'sync_transaction_version': '1', 'children':...  13109174777723357   \n",
       "1   [{'sync_transaction_version': '1', 'name': '10...  13109174777705078   \n",
       "2   [{'sync_transaction_version': '1', 'name': 'Vi...  13151775038046138   \n",
       "3   [{'sync_transaction_version': '1', 'children':...  13109116258767161   \n",
       "4   [{'sync_transaction_version': '1', 'name': '10...  13109116258767360   \n",
       "5   [{'sync_transaction_version': '1', 'name': 'Bi...  13109116258767557   \n",
       "6   [{'sync_transaction_version': '1', 'children':...  13109116258768141   \n",
       "7   [{'sync_transaction_version': '1', 'children':...  13109116258768527   \n",
       "8   [{'sync_transaction_version': '1', 'children':...  13109174778060091   \n",
       "9   [{'sync_transaction_version': '1', 'children':...  13109174777774261   \n",
       "10  [{'sync_transaction_version': '1', 'children':...  13109174777787223   \n",
       "11                                                NaN  13052003813434881   \n",
       "12                                                NaN  13063834756373080   \n",
       "13                                                NaN  12922401002026674   \n",
       "14                                                NaN  13040939957191370   \n",
       "15                                                NaN  13015966990995159   \n",
       "16                                                NaN  13028410021875400   \n",
       "17                                                NaN  13129671690278805   \n",
       "18                                                NaN  13013887684506372   \n",
       "19                                                NaN  13109352631895116   \n",
       "\n",
       "        date_modified    id                                      meta_info  \\\n",
       "0   13117039265083786  1203                                            NaN   \n",
       "1   13170097765743670  1137                                            NaN   \n",
       "2   13163106408774757  3688                                            NaN   \n",
       "3   13167924695837332     8                                            NaN   \n",
       "4   13169571777752619     9                                            NaN   \n",
       "5   13164750190698910    10                                            NaN   \n",
       "6   13169487040373303    13                                            NaN   \n",
       "7   13146764550541136    15                                            NaN   \n",
       "8   13167438541781746  1617                                            NaN   \n",
       "9   13168624772525920  1399                                            NaN   \n",
       "10  13169689046946971  1455                                            NaN   \n",
       "11                NaN  2481  {'last_visited_desktop': '13134580082653860'}   \n",
       "12                NaN  2482                                            NaN   \n",
       "13                NaN  2483  {'last_visited_desktop': '13170027462729742'}   \n",
       "14                NaN  2484  {'last_visited_desktop': '13166057563293141'}   \n",
       "15                NaN  2485                                            NaN   \n",
       "16                NaN  2487  {'last_visited_desktop': '13164602768571434'}   \n",
       "17                NaN  2911  {'last_visited_desktop': '13166057638023431'}   \n",
       "18                NaN  2488                                            NaN   \n",
       "19                NaN  2516  {'last_visited_desktop': '13164416977631716'}   \n",
       "\n",
       "                                           name sync_transaction_version  \\\n",
       "0                                         Perso                        1   \n",
       "1                                          News                        1   \n",
       "2                                      Accounts                        1   \n",
       "3                                          CUNY                        1   \n",
       "4                             Resources & Tools                        1   \n",
       "5                           Courses & Textbooks                        1   \n",
       "6                         Science sites & tools                        1   \n",
       "7                                          Misc                        1   \n",
       "8                                         Progg                        1   \n",
       "9                                          Jobs                        1   \n",
       "10                                       People                        1   \n",
       "11                               Amazon Student                        1   \n",
       "12                                NYT Chronicle                        1   \n",
       "13                         BetterWorldBooks.com                     1203   \n",
       "14  Weather Forecast New York City - Foreca.com                        1   \n",
       "15                 Local time in France – Paris                        1   \n",
       "16                    National Hurricane Center                        1   \n",
       "17                                  weather.gov                        1   \n",
       "18                           USGS - Earthquakes                        1   \n",
       "19                                 Météo-France                        1   \n",
       "\n",
       "      type                                                url  \n",
       "0   folder                                                NaN  \n",
       "1   folder                                                NaN  \n",
       "2   folder                                                NaN  \n",
       "3   folder                                                NaN  \n",
       "4   folder                                                NaN  \n",
       "5   folder                                                NaN  \n",
       "6   folder                                                NaN  \n",
       "7   folder                                                NaN  \n",
       "8   folder                                                NaN  \n",
       "9   folder                                                NaN  \n",
       "10  folder                                                NaN  \n",
       "11     url  http://www.amazon.com/gp/student/signup/info/r...  \n",
       "12     url  http://chronicle.nytlabs.com/?keyword=Bush.Nix...  \n",
       "13     url                   http://www.betterworldbooks.com/  \n",
       "14     url  http://www.foreca.com/United_States/New_York/N...  \n",
       "15     url  http://www.timeanddate.com/worldclock/city.htm...  \n",
       "16     url                           http://www.nhc.noaa.gov/  \n",
       "17     url                        http://www.weather.gov/phi/  \n",
       "18     url  http://earthquake.usgs.gov/earthquakes/?source...  \n",
       "19     url  http://www.meteofrance.com/previsions-meteo-fr...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DictQuery(dict):\n",
    "    def get(self, path, default = None):\n",
    "        keys = path.split(\"/\")\n",
    "        val = None\n",
    "\n",
    "        for key in keys:\n",
    "            if val:\n",
    "                if isinstance(val, list):\n",
    "                    val = [ v.get(key, default) if v else None for v in val]\n",
    "                else:\n",
    "                    val = val.get(key, default)\n",
    "            else:\n",
    "                val = dict.get(self, key, default)\n",
    "\n",
    "            if not val:\n",
    "                break;\n",
    "\n",
    "        return val\n",
    "    \n",
    "            \n",
    "#Now you can do this:\n",
    "for item in animals:\n",
    "    print DictQuery(item).get(\"animal/type\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def walk_json(tree, path=[]):\n",
    "    try:\n",
    "        for root, child in tree.items():\n",
    "            yield from walk_json(child, path + [root])\n",
    "    except AttributeError: # in case .items() is not possible (on leaves)\n",
    "        yield path + [tree]\n",
    "        \n",
    "list(walk_json(data['roots']['other']['children']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/45168524/deeply-nested-json-response-to-pandas-dataframe\n",
    "\n",
    "def dict_generator(indict, pre=None):\n",
    "    pre = pre[:] if pre else []\n",
    "    if isinstance(indict, dict):\n",
    "        for key, value in indict.items():\n",
    "            if isinstance(value, dict):\n",
    "                for d in dict_generator(value, pre + [key]):\n",
    "                    yield d\n",
    "            elif isinstance(value, list) or isinstance(value, tuple):\n",
    "                for v in value:\n",
    "                    for d in dict_generator(v, pre = [key]):\n",
    "                        yield d\n",
    "            else:\n",
    "                yield pre + [key, value]\n",
    "    else:\n",
    "        yield indict       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "new_string = []\n",
    "for line in session.execute('select json ....'):\n",
    "    new_string.append(json.loads(line.json))\n",
    "\"\"\"\n",
    "\n",
    "cols = ['ID', 'criteria', 'type', 'name', 'value']\n",
    "\n",
    "rows = []\n",
    "for line in data:\n",
    "    data_id = line['ID']\n",
    "    criteria = line['profile']['criteria']\n",
    "    for d in criteria:\n",
    "        rows.append([data_id, criteria.index(d)+1, *list(d.values())[:-1]])\n",
    "\n",
    "df = pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pandas.io.json.json_normalize(data, record_path=None, \n",
    "                              meta=None, meta_prefix=None, \n",
    "                              record_prefix=None, errors='raise', sep='.')\n",
    "\n",
    "data : dict or list of dicts; Unserialized JSON objects\n",
    "record_path : string or list of strings, default None; Path in each object to list of records. \n",
    "              If not passed, data will be assumed to be an array of records\n",
    "\n",
    "meta : list of paths (string or list of strings), default None;\n",
    "       Fields to use as metadata for each record in resulting table\n",
    "\n",
    "record_prefix : string, default None\n",
    "                If True, prefix records with dotted (?) path, e.g. foo.bar.field if path to records is [‘foo’, ‘bar’]\n",
    "\n",
    "meta_prefix : string, default None\n",
    "\n",
    "errors : {‘raise’, ‘ignore’}, default ‘raise’\n",
    "         ‘ignore’ : will ignore KeyError if keys listed in meta are not always present\n",
    "         ‘raise’ : will raise KeyError if keys listed in meta are not always present\n",
    "          New in version 0.20.0.\n",
    "\n",
    "sep : string, default ‘.’\n",
    "      Nested records will generate names separated by sep, e.g., for sep=’.’, { ‘foo’ : { ‘bar’ : 0 } } -> foo.bar\n",
    "      New in version 0.20.0.\n",
    "      \n",
    "json_normalize(raw, [['_source', 'authors']], ['_id', ['_source', 'journal'], ['_source', 'title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw =[ {'countries':'North Hemisphere'}, \n",
    "       {'Members': [{'country':'USA'},\n",
    "                        {'components': [ {'state': 'Florida', \n",
    "                                        'shortname': 'FL',\n",
    "                                        'info': { 'governor': 'Rick Scott' },\n",
    "                                        'counties': [{'name': 'Dade', 'population': 12345},\n",
    "                                                  {'name': 'Broward', 'population': 40000},\n",
    "                                                  {'name': 'Palm Beach', 'population': 60000}]},\n",
    "                                        {'state': 'Ohio',\n",
    "                                         'shortname': 'OH',\n",
    "                                         'info': {'governor': 'John Kasich'},\n",
    "                                         'counties': [{'name': 'Summit', 'population': 1234},\n",
    "                                                      {'name': 'Cuyahoga', 'population': 1337}]}]\n",
    "                         },\n",
    "                    {'country':'France'},\n",
    "                         {'components': [ {'state': 'Bretagne', \n",
    "                                         'shortname': 'bre',\n",
    "                                         'info': { 'governor': 'Rick Marcel' },\n",
    "                                         'counties': [{'name': 'villeun', 'population': 12345},\n",
    "                                                      {'name': 'deux', 'population': 40000},\n",
    "                                                      {'name': 'trois', 'population': 60000}]},\n",
    "                                          {'state': 'Bourgogne',\n",
    "                                         'shortname': 'bou',\n",
    "                                         'info': {'governor': 'Martin Kasich'},\n",
    "                                         'counties': [{'name': 'bou1', 'population': 1234},\n",
    "                                                      {'name': 'bou2', 'population': 1337}]}]\n",
    "                         }]\n",
    "       }]\n",
    "\n",
    "#result = json_normalize(raw, record_path=[['components']], meta=['state', 'shortname', ['info', 'governor']])\n",
    "#result = json_normalize(raw, record_path=['Members']) #, record_path=['Members'], meta=['components', 'state', 'shortname', ['info', 'governor']])\n",
    "#, [['_source', 'authors']], ['_id', ['_source', 'journal'], ['_source', 'title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fields': [{'name': 'index', 'type': 'integer'},\n",
       "  {'name': 'Members', 'type': 'string'},\n",
       "  {'name': 'countries', 'type': 'string'}],\n",
       " 'pandas_version': '0.20.0',\n",
       " 'primaryKey': ['index']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame( raw)\n",
    "schem = pd.io.json.build_table_schema(df);\n",
    "schem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def generate_children(tree):\n",
    "    queue = deque()\n",
    "    queue.append((tree, None))\n",
    "\n",
    "    while queue:\n",
    "        node, parent = queue.pop()\n",
    "        children = []\n",
    "        for child in node['children']:\n",
    "            queue.append((child, node['id']))\n",
    "            children.append(child['id'])\n",
    "        yield node['id'], parent, children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import matlplotlib.pyplot as plt\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Blog post: Machine Learning, NLP: Text Classification using scikit-learn, python and NLTK.\n",
    "#            https://github.com/javedsha/text-classification\n",
    "#\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "#Loading the data set - training data.\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "# You can check the target names (categories) and some data files by following commands.\n",
    "twenty_train.target_names #prints all the categories\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])) #prints first line of the first data file\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "# Extracting features from text files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "# Machine Learning: Training Naive Bayes (NB) classifier on training data.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "# Building a pipeline: \n",
    "# We can write less code and do all of the above, by building a pipeline as follows:\n",
    "# The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "\t\t\t\t\t ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "# Performance of NB Classifier\n",
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), \n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', \n",
    "\t\t\t\t\t\t                           penalty='l2',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   alpha=1e-3, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   n_iter=5, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   random_state=42)\n",
    "\t\t\t\t\t\t )\n",
    "\t\t\t\t\t\t])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted_svm == twenty_test.target)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "# Performance tuning using Grid Search\n",
    "# Here, we are creating a list of parameters for which we would like to do performance tuning. \n",
    "# All the parameters name start with the classifier name (remember the arbitrary name we gave). \n",
    "# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which \n",
    "# is optimal.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], \n",
    "              'tfidf__use_idf': (True, False), \n",
    "\t\t\t  'clf__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "# Next, we create an instance of the grid search by passing the classifier, parameters \n",
    "# and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# See the best mean score and the params:\n",
    "gs_clf.best_score_\n",
    "gs_clf.best_params_\n",
    "\n",
    "# Output for above should be: \n",
    "#  The accuracy has now increased to ~90.6% for the NB classifier, and the corresponding parameters \n",
    "#  are {‘clf__alpha’: 0.01, \n",
    "#       ‘tfidf__use_idf’: True, \n",
    "#\t\t‘vect__ngram_range’: (1, 2)}.\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "# Similarly doing grid search for SVM\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], \n",
    "                  'tfidf__use_idf': (True, False),\n",
    "\t\t\t\t  'clf-svm__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "# NLTK\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "# Removing stop words\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "# Stemming Code\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), \n",
    "                             ('tfidf', TfidfTransformer()), \n",
    "                             ('mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted_mnb_stemmed == twenty_test.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Create simple data: Try to differentiate the two first classes of the iris data\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "\n",
    "\n",
    "# [ A ] In binary classification settings ********************************\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# Limit to the two first classes, and split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "# Create a simple classifier\n",
    "classifier = svm.LinearSVC(random_state=random_state)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_score = classifier.decision_function(X_test)\n",
    "\n",
    "# Compute the average precision score\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "\n",
    "\n",
    "# Plot the Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class PR curve: mean P={0:0.2f}'.format(average_precision))\n",
    "\n",
    "\n",
    "# [ B ] In multi-label classification settings ********************************\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "#from sklearn.metrics import precision_recall_curve\n",
    "#from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Create multi-label data, fit, and predict\n",
    "\n",
    "# We create a multi-label dataset, to illustrate PR in multi-label settings:\n",
    "# Use label_binarize to be multi-label like settings\n",
    "\n",
    "Y = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = Y.shape[1]\n",
    "\n",
    "# Split into training and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "# We use OneVsRestClassifier for multi-label prediction\n",
    "classifier = OneVsRestClassifier(svm.LinearSVC(random_state=random_state))\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "y_score = classifier.decision_function(X_test)\n",
    "\n",
    "\n",
    "# The average precision score in multi-label settings\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i], y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n",
    "\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly:\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(), y_score.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test, y_score, average=\"micro\")\n",
    "\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "# Plot the micro-averaged Precision-Recall curve:\n",
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall[\"micro\"], precision[\"micro\"], step='post', alpha=0.2, color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Aver. precision score, micro-averaged over all classes: mean P={0:0.2f}'\n",
    "          .format(average_precision[\"micro\"]))\n",
    "\n",
    "\n",
    "# Plot Precision-Recall curve for each class and iso-f1 curves\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "# setup plot details\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\t\t  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [mlpy]",
   "language": "python",
   "name": "Python [mlpy]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
